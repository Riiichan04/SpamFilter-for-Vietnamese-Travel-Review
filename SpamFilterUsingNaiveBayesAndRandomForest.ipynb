{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyzx4d1KWsfo"
   },
   "source": [
    "### Mount vào dir chứa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QE85lzsHSGL1",
    "outputId": "09b1d1b5-fb56-4536-a7d6-171c9d477c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/MyDrive/ModelAI/SpamFilter\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd '/content/gdrive/MyDrive/ModelAI/SpamFilter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iania4z5WpBv"
   },
   "source": [
    "### Cài các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whm2l32sSmut",
    "outputId": "c2fe1055-8613-493f-aa8d-e82b856c1e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Collecting underthesea\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.12.14)\n",
      "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n",
      "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyRT65LgVNfK"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5dB3BPvU4i1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DIR_ROOT = '/content/gdrive/MyDrive/ModelAI/SpamFilter'\n",
    "DIR_DATASET = os.path.join(DIR_ROOT, 'dataset')\n",
    "\n",
    "\n",
    "PATH_TRAIN = os.path.join(DIR_DATASET, 'train.csv')\n",
    "PATH_DEV = os.path.join(DIR_DATASET, 'dev.csv')\n",
    "PATH_TEST = os.path.join(DIR_DATASET, 'test.csv')\n",
    "\n",
    "\n",
    "STOPWORDS_PATH = os.path.join(DIR_ROOT, 'vietnamese-stopwords.txt')\n",
    "\n",
    "\n",
    "# Tạo dir model nếu chưa có\n",
    "MODEL_DIR = os.path.join(DIR_ROOT, 'model')\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUHXLAPLXmRD"
   },
   "source": [
    "### Load các từ trong tập train và tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0XuJ0OZXl8i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Lấy các tập train và test\n",
    "data_train = pd.read_csv(PATH_TRAIN)\n",
    "data_test = pd.read_csv(PATH_TEST)\n",
    "\n",
    "\n",
    "# Tạo tập train\n",
    "x_train = data_train['Comment']\n",
    "y_train = data_train['Label']\n",
    "# Fill các trường null (nếu có) trong dataset\n",
    "x_train = x_train.fillna('')    #Câu không có nội dung\n",
    "y_train = y_train.fillna(1)     #Gán nhãn 1 nếu chưa có nhãn\n",
    "\n",
    "\n",
    "# Tạo tập test\n",
    "x_test = data_test['Comments']\n",
    "y_test = data_test['Label']\n",
    "\n",
    "\n",
    "# Tạo list stopword cho tiền xử lý\n",
    "with open(STOPWORDS_PATH, 'r') as stopword_txt:\n",
    "    list_stopwords = []\n",
    "    for line in stopword_txt:\n",
    "        #Đọc từng dòng trong txt và cho vào list\n",
    "        word = line.strip('\\n')\n",
    "        list_stopwords.append(word)\n",
    "    #Chuyển list stopword thành set\n",
    "    list_stopwords = set(list_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVhqwqrzWkjH"
   },
   "source": [
    "### Tiền xử lý dữ liệu với `underthesea`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGVd0VTSW1Wt"
   },
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "import string, re\n",
    "\n",
    "\n",
    "def filter_stop_word(tokenizers, list_stopwords):\n",
    "    return [token for token in tokenizers if token not in list_stopwords]\n",
    "\n",
    "\n",
    "# Lọc hết các emoji khỏi câu\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "def pre_processing_data(text):\n",
    "    if type(text) is not str: return text\n",
    "    \n",
    "    #Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "    #Loại bỏ dấu câu\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    #Tách từ thành list token\n",
    "    tokenizers = word_tokenize(text)\n",
    "    #Lọc stopword\n",
    "    tokenizers = filter_stop_word(tokenizers, list_stopwords)\n",
    "    #Tạo câu hoàn chỉnh dựa trên stopword\n",
    "    sentences = \" \".join(tokenizers)\n",
    "    #Loại bỏ hết emoji\n",
    "    sentences = remove_emoji(sentences)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjlIO1ffoQd7"
   },
   "source": [
    "### Tiến hành train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bQagsDIMOyi",
    "outputId": "2d65b73e-718c-4d5d-8734-78244e145ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác ở tập test: 0.9441249864381035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      5231\n",
      "           1       0.96      0.91      0.93      3986\n",
      "\n",
      "    accuracy                           0.94      9217\n",
      "   macro avg       0.95      0.94      0.94      9217\n",
      "weighted avg       0.94      0.94      0.94      9217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import pipeline để chạy vertorize và sau đó xử lý bằng các thuật toán đã chọn\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#Vectorize hóa các từ trong văn bản bằng Bag of Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#Import thuật toán Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "#RandomForest và Stacking để ghép 2 thuật toán lại\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier \n",
    "#Dùng để kiểm tra lại model\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "#Import các hàm để đánh giá\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Tiến hành tiền xử lý tập train\n",
    "x_train = x_train.apply(pre_processing_data)\n",
    "\n",
    "\n",
    "#Các classifier sử dụng:\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=1000)\n",
    "nb = MultinomialNB(class_prior=[0.5, 0.5])\n",
    "\n",
    "\n",
    "#Kết hợp lại (Dùng Logistic Regression làm meta)\n",
    "stacking = StackingClassifier(estimators=[('nb', nb), ('rf', rf)], final_estimator=LogisticRegression())\n",
    "\n",
    "\n",
    "# Tạo pipeline để quy định các bước train\n",
    "pline = Pipeline([\n",
    "    # Vectorize hóa input bằng Bag of Word\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 10), max_features=85000)),\n",
    "    # Tiến hành phân loại (classifier) bằng việc kết hợp 2 thuật toán\n",
    "    ('classifier', stacking)\n",
    "])\n",
    "\n",
    "\n",
    "#Tiến hành train\n",
    "pline.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred_test = pline.predict(x_test)\n",
    "\n",
    "\n",
    "# Hiển thị kết quả sau khi test\n",
    "print(\"Độ chính xác ở tập test:\", accuracy_score(y_test, y_pred_test))\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiSetThTA2-C"
   },
   "source": [
    "### Đóng gói model lại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuuGi365A4ee"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lưu model vào tệp .pkl\n",
    "with open(os.path.join(MODEL_DIR, 'spamfilter_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(pline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpBNxowFCUNH"
   },
   "source": [
    "#### Cách sử dụng model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPUV5D3EB3U3",
    "outputId": "e4796aac-8a19-4051-af36-d6112eee226a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98947216 0.01052784]]\n"
     ]
    }
   ],
   "source": [
    "# Tải lại model từ tệp .pkl\n",
    "with open(os.path.join(MODEL_DIR, 'spamfilter_model.pkl'), 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "\n",
    "# Sử dụng model đã tải để dự đoán\n",
    "y_predict_loaded = loaded_pipeline.predict_proba([\"Nhà này thật tuyệt vời, chủ nhà rất lịch sự. Chắc chắn sẽ ở lại đây lần nữa!!\"])\n",
    "print(y_predict_loaded)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
