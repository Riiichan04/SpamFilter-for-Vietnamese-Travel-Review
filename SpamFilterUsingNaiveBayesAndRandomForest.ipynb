{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Mount vào dir chứa model"
   ],
   "metadata": {
    "id": "jyzx4d1KWsfo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd '/content/gdrive/MyDrive/ModelAI/SpamFilter'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QE85lzsHSGL1",
    "outputId": "09b1d1b5-fb56-4536-a7d6-171c9d477c7f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/MyDrive/ModelAI/SpamFilter\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cài các thư viện cần thiết"
   ],
   "metadata": {
    "id": "Iania4z5WpBv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install scikit-learn underthesea"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whm2l32sSmut",
    "outputId": "c2fe1055-8613-493f-aa8d-e82b856c1e9a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Collecting underthesea\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.12.14)\n",
      "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.9/20.9 MB\u001B[0m \u001B[31m71.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m657.8/657.8 kB\u001B[0m \u001B[31m39.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m57.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n",
      "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset"
   ],
   "metadata": {
    "id": "HyRT65LgVNfK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "DIR_ROOT = '/content/gdrive/MyDrive/ModelAI/SpamFilter'\n",
    "DIR_DATASET = os.path.join(DIR_ROOT, 'dataset')\n",
    "\n",
    "PATH_TRAIN = os.path.join(DIR_DATASET, 'train.csv')\n",
    "PATH_DEV = os.path.join(DIR_DATASET, 'dev.csv')\n",
    "PATH_TEST = os.path.join(DIR_DATASET, 'test.csv')\n",
    "\n",
    "STOPWORDS_PATH = os.path.join(DIR_ROOT, 'vietnamese-stopwords.txt')\n",
    "\n",
    "\n",
    "# Tạo dir model nếu chưa có\n",
    "MODEL_DIR = os.path.join(DIR_ROOT, 'model')\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ],
   "metadata": {
    "id": "p5dB3BPvU4i1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load các từ trong tập train và tập test"
   ],
   "metadata": {
    "id": "CUHXLAPLXmRD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv(PATH_TRAIN)\n",
    "data_test = pd.read_csv(PATH_TEST)\n",
    "vnbooking_test = pd.read_csv(os.path.join(DIR_ROOT, 'vnbooking_dataset/train.csv'))\n",
    "bnb_train_data = pd.read_csv(os.path.join(DIR_ROOT, 'bnb_dataset/translated_reviews.csv'))\n",
    "\n",
    "# Tạo tập test từ vnbooking\n",
    "x_test_bnb = vnbooking_test['Comments']\n",
    "y_test_bnb = vnbooking_test['Label']\n",
    "y_test_bnb.replace(2, 1, inplace=True)\n",
    "\n",
    "\n",
    "# Chuẩn hóa data\n",
    "y_test_replace = data_test['Label']\n",
    "y_test_replace.replace(0, 1, inplace=True)\n",
    "y_test_replace.replace(2, 1, inplace=True)\n",
    "# Tạo tập test cuối cùng\n",
    "x_test_train = pd.concat([data_test['Comment'], x_test_bnb], ignore_index=True)\n",
    "y_test_train = pd.concat([y_test_replace, y_test_bnb], ignore_index=True)\n",
    "\n",
    "\n",
    "# Tạo tập train (nhận hết các spam)\n",
    "x_spam_train = data_train[data_train['Label'] == 1]['Comment']\n",
    "y_spam_train = data_train[data_train['Label'] == 1]['Label']\n",
    "# Tập train\n",
    "bnb_x_train = bnb_train_data['Comments']\n",
    "bnb_y_train = bnb_train_data['Label']\n",
    "\n",
    "# Hoàn chỉnh tập train\n",
    "complete_x_train = pd.concat([x_spam_train, bnb_x_train], ignore_index=True)\n",
    "complete_y_train = pd.concat([y_spam_train, bnb_y_train], ignore_index=True)\n",
    "complete_x_train = complete_x_train.fillna('')\n",
    "complete_y_train = complete_y_train.fillna(1)\n",
    "\n",
    "with open(STOPWORDS_PATH, 'r') as stopword_txt:\n",
    "    list_stopwords = []\n",
    "    for line in stopword_txt:\n",
    "        #Đọc từng dòng trong txt và cho vào list\n",
    "        word = line.strip('\\n')\n",
    "        list_stopwords.append(word)\n",
    "    #Chuyển list stopword thành set\n",
    "    list_stopwords = set(list_stopwords)"
   ],
   "metadata": {
    "id": "L0XuJ0OZXl8i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tiền xử lý dữ liệu với `underthesea`"
   ],
   "metadata": {
    "id": "tVhqwqrzWkjH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from underthesea import word_tokenize, pos_tag\n",
    "import string, re\n",
    "\n",
    "\n",
    "def filter_stop_word(tokenizers, list_stopwords):\n",
    "    return [token for token in tokenizers if token not in list_stopwords]\n",
    "\n",
    "\n",
    "# Lọc hết các emoji khỏi câu\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "def pre_processing_data(text):\n",
    "    if type(text) is not str: return text\n",
    "    #Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "    #Loại bỏ dấu câu\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    #Tách từ thành list token\n",
    "    tokenizers = word_tokenize(text)\n",
    "    #Lọc stopword\n",
    "    tokenizers = filter_stop_word(tokenizers, list_stopwords)\n",
    "    #Tạo câu hoàn chỉnh dựa trên stopword\n",
    "    sentences = \" \".join(tokenizers)\n",
    "    #Loại bỏ hết emoji\n",
    "    sentences = remove_emoji(sentences)\n",
    "    #Tạm bỏ qua lemmetazion\n",
    "    return sentences"
   ],
   "metadata": {
    "id": "BGVd0VTSW1Wt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tiến hành train"
   ],
   "metadata": {
    "id": "HjlIO1ffoQd7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Import pipeline để chạy\n",
    "from sklearn.pipeline import Pipeline\n",
    "#Import các thư viện cần thiết\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #Vectorize hóa các từ trong văn bản\n",
    "from sklearn.naive_bayes import MultinomialNB #Tiến hành dùng Naive Bayes để xử lý\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Import các hàm để đánh giá\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "complete_x_train = complete_x_train.apply(pre_processing_data)\n",
    "x_test_bnb = x_test_bnb.apply(pre_processing_data)\n",
    "\n",
    "#Các classifier sử dụng:\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=1000)\n",
    "nb = MultinomialNB(class_prior=[0.5, 0.5])\n",
    "#Kết hợp lại (Dùng thử Regression làm meta)\n",
    "stacking = StackingClassifier(estimators=[('nb', nb), ('rf', rf)], final_estimator=LogisticRegression())\n",
    "\n",
    "pline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 10), max_features=85000)),  #Dùng Bag of Word\n",
    "    ('classifier', stacking)\n",
    "])\n",
    "\n",
    "\n",
    "#Tiến hành train\n",
    "pline.fit(complete_x_train, complete_y_train)\n",
    "\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred_test = pline.predict(x_test_train)\n",
    "print(\"Độ chính xác ở tập test:\", accuracy_score(y_test_train, y_pred_test))\n",
    "print(classification_report(y_test_train, y_pred_test))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bQagsDIMOyi",
    "outputId": "2d65b73e-718c-4d5d-8734-78244e145ef6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Độ chính xác ở tập test: 0.9441249864381035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      5231\n",
      "           1       0.96      0.91      0.93      3986\n",
      "\n",
      "    accuracy                           0.94      9217\n",
      "   macro avg       0.95      0.94      0.94      9217\n",
      "weighted avg       0.94      0.94      0.94      9217\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Đóng gói model lại"
   ],
   "metadata": {
    "id": "OiSetThTA2-C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# Lưu model vào tệp .pkl\n",
    "with open(os.path.join(MODEL_DIR, 'spamfilter_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(pline, f)"
   ],
   "metadata": {
    "id": "KuuGi365A4ee"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Mở model lên"
   ],
   "metadata": {
    "id": "BpBNxowFCUNH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# Tải lại model từ tệp .pkl\n",
    "with open(os.path.join(MODEL_DIR, 'spamfilter_model.pkl'), 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "# Sử dụng model đã tải để dự đoán\n",
    "y_predict_loaded = loaded_pipeline.predict_proba([\"Thời gian lưu trú của chúng tôi với Hội tốt hơn 1000 lần so với những gì tôi tưởng tượng. Cảnh tượng và cảm giác của các ngọn núi xung quanh cùng với âm thanh vang vọng của gà trống gáy đã để lại ấn tượng lâu dài. Hội đãi chúng tôi những bữa ăn tuyệt vời ba lần một ngày cùng với trái cây mới hái từ cây mọc ở sân xung quanh. Sau khi nhận được những đề xuất tuyệt vời về các điểm tham quan trong khu vực, Hội đã cung cấp một dịch vụ vận chuyển tuyệt vời để đưa chúng tôi đến bất cứ nơi nào chúng tôi chọn. Bất cứ ai đang tìm kiếm một nơi thư giãn thanh bình, cùng với dịch vụ tuyệt vời, Yen Binh Homestay nằm ở đầu danh sách nơi ở của tôi.\"])\n",
    "print(y_predict_loaded)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPUV5D3EB3U3",
    "outputId": "e4796aac-8a19-4051-af36-d6112eee226a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.98947216 0.01052784]]\n"
     ]
    }
   ]
  }
 ]
}
